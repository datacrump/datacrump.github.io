{"pageProps":{"post":{"slug":"storage","category":{"slug":"basics","title":"Basics","excerpt":"Basic stuff which you already know","coverImage":"/assets/logo.png","content":"","top":true},"title":"Object vs block store","excerpt":"Cheap, scalable, and yet is it really the best storage type for your next big data project","coverImage":"/assets/basics/storage/storage_cover.png","cardImage":"/assets/basics/storage/storage_card.png","date":"2022-10-13T12:36:35.000Z","contentPath":"/basics/storage.md","content":"<h2>what is</h2>\n<h3>object storage</h3>\n<p>AWS simple storage service (S3) - is the best know object-store. However, it’s not the only provider of object storage, other choices:</p>\n<ul>\n<li>Azure Blob storage</li>\n<li>Google cloud storage (GCS)</li>\n<li>DigitalOcean spaces</li>\n</ul>\n<blockquote>\n<p>search for object store providers you might have found one in your home town :)</p>\n</blockquote>\n<h3>block storage</h3>\n<p>I think the easiest way to think about block storage - it's an HDD or SSD or NVRAM which you can attach to your running instance in the cloud. Each cloud provider does have a version of it.</p>\n<h2>usage</h2>\n<p>If your project is Netflix, Spotify, or a book library - in which your objects need to be returned to the user full - object store might be a perfect solution.</p>\n<h3>object store for database</h3>\n<blockquote>\n<p>amount of data you need to read is directly responsible for your database performance</p>\n</blockquote>\n<p>The parquet file is split into row groups which contain columns and meta information about it. Readers are expected to first read the file metadata to find all the column chunks they are interested in. The column chunks should then be read sequentially. The format is explicitly designed to separate the metadata from the data.</p>\n<p>However to achieve this storage needs to support the ability to read chunks of files, this is easily achievable in block storage because the operating system supports this.</p>\n<p>Object storage on the other hand returns all or nothing for a given key. In S3 there is a way to seek, however HTTP protocol is not as fast as direct access storage provided by the operating system.</p>\n<p>At least right now many databases which are based on object storage - first need to download a file to local storage (block storage) and then process it the same way.</p>\n<p>Of course, this is not the case if your files are JSON or CSV. The computation engine needs to read a full file to be able to parse information from it. If you need to read the full file object store is a very good solution.</p>\n<h2>which one to chooce</h2>\n<p>for near realtime data retrieval - block storage</p>\n<p>for data lake or unstructured data - object store</p>\n<h2>additional info</h2>\n<h3>pricing</h3>\n<p>Block storage at a minimum twice expensive compared to object-store.</p>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Product</th>\n<th>Price per TB / month</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Object</td>\n<td>AWS S3</td>\n<td>around $23</td>\n</tr>\n<tr>\n<td>Object</td>\n<td>GCS</td>\n<td>around $23</td>\n</tr>\n<tr>\n<td>Block</td>\n<td>EBS</td>\n<td>starts at $80</td>\n</tr>\n<tr>\n<td>Block</td>\n<td>Google Disk</td>\n<td>starts at $44</td>\n</tr>\n</tbody>\n</table>\n<h3>access layer</h3>\n<p>Object sore - HTTP protocol</p>\n<p>Block storage - operating system</p>\n<h2>summary</h2>\n<p>Both object and block storage has it's use cases and very often you will need to go with a mixture of both</p>\n<blockquote>\n<p>what fits everything very often doesn't fit anything</p>\n</blockquote>\n","top":true,"visible":true,"tags":[{"slug":"aws","title":"AWS","excerpt":"","coverImage":"/assets/logo.png","content":""},{"slug":"storage","title":"Storage","excerpt":"","coverImage":"/assets/logo.png","content":""}]},"category":{"slug":"basics","title":"Basics","excerpt":"Basic stuff which you already know","coverImage":"/assets/logo.png","content":"","top":true},"posts":[{"slug":"file-formats","category":{"slug":"basics","title":"Basics","excerpt":"Basic stuff which you already know","coverImage":"/assets/logo.png","content":"","top":true},"title":"File formats","excerpt":"I was surpsided by results then comparing Parquet, Avro, JSON and CSV for both storage and speed","coverImage":"/assets/basics/file-formats/format_cover.png","cardImage":"/assets/basics/file-formats/format_card.png","date":"2022-10-20T20:36:35.000Z","contentPath":"/basics/file-formats.md","content":"## tl;tr\n\nEach file format has its own place in data engineering. You should choose one based on your project or workload.\n\n\n## preparing my data\n\nI have pulled my data set from [here](https://data.openaddresses.io/runs/1195253/lt/countrywide.zip)\n\nFor my testing, I'll be using only a couple of columns so I have transformed the original file format to a new one.\n\n```python\n{\n    'country': 'Lithuania',\n    'city': row['CITY'],\n    'postcode': int(row['POSTCODE'].replace(\"LT-\", \"\") if row['POSTCODE'] != ' ' else 0),\n    'street': f\"{' '.join(row['STREET'].split(' ')[1:])} {row['STREET'].split(' ')[0]}\",\n    'full_address': build_full_address(row),\n    'longitude': float(row['LON']),\n    'latitude': float(row['LAT'])\n}\n```\n\nI have added full_address column which joins street, city, and postcode.\n\nData set has 1036250 rows\n\n## generating data files\n\n### same dataset different formats\n\nIn the first step I took the original data and wrote it into different file formats:\n\n| Format | Size  | \n| -----| ------------ | \n| csv | 106MB   | \n| json | 222MB     |\n| avro | 24MB    | \n| parquet | 32MB     |\n\nBoth CSV and JSON are losing a lot compared to Avro and Parquet, however, this is expected because both Avro and Parquet are binary formats (they also use compression) while CSV and JSON are not compressed.\n\nTo make this more comparable I will be applying compression for both JSON and CSV.\n\nCompression makes a difference\n\n| Format | Original  | Compressed|\n| -----| ------------ | --|\n| csv | 106MB   | 20MB\n| json | 222MB     |22MB|\n| avro | 24MB     | 24MB|\n| parquet | 32MB     | 32MB|\n\nMoving forward I'll be using compressed files for comparison\n\n### shuffle some data\n\nMy initial data set was already sorted, I will shuffle it to make a more realistic.\n\n| Format | Original  | Shuffle | \n| -----| ------------ | --|\n| csv | 20MB| 34MB   | \n| json | 22MB| 40MB  |\n| avro | 24MB| 45MB   |\n| parquet | 32MB | 44MB|\n\n![creat bucket](/assets/basics/file-formats/size.png)\n\nShuffle has affected the overall size for all formats. I surprised it Avro has the most significant impact. It increased by 88%. Parquet is a clear winner here with data increase only by 38%\n\n![creat bucket](/assets/basics/file-formats/increase.png)\n\n### 5x\n\nFor speed testing, I wanted to have more data. So I replicate data 5 times and the final dataset was again shuffled.\n\n| Format | Original  | Shuffle | 5x |\n| -----| ------------ | --| --|\n| csv | 20MB| 34MB   | 168MB |\n| json | 22MB| 40MB  | 198MB |\n| avro | 24MB| 45MB   | 222MB |\n| parquet | 32MB | 44MB| 215MB |\n\n\n> No surprises here... \n\n## query speed\n\nFor my testing, I'm using Apache Spark. I load all data types into the data frame and register it as TempView. \n\n> we will be testing queries... we must have tables and SQL!!!\n\n```python\n#parquet\ndf = spark.read.parquet('data/parquet/data_5x.parquet')\ndf.createOrReplaceTempView('source_table')\n\n#csv\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load('data/csv/data_5x.csv.gz')\ndf.createOrReplaceTempView('source_table')\n\n#json\ndf = spark.read.option(\"multiline\", \"true\").json(\"data/json/data_5x.json.gz\")\ndf.createOrReplaceTempView('source_table')\n\n#avro\ndf = spark.read.format(\"avro\").load(\"data/avro/data_5x.avro\")\ndf.createOrReplaceTempView('source_table')\n```\n\nI came up with a list of queries.\n\n> if you have a query suggestion - please leave it in the comments on social media.\n\nqueries:\n```SQL\n#Q1: \nselect count(*) as cnt from source_table\n\n#Q2: \nselect count(distinct full_address) as cnt from source_table\n\n#Q3:\nselect count(distinct full_address) as cnt from source_table where city = 'Vilnius'\n\n#Q4:\nselect city, count(distinct full_address) as cnt from source_table group by 1 order by cnt desc\n\n#Q5:\nselect count(*) from source_table where city = 'Vilnius' and street = 'Kauno gatvė'\n\n#Q6:\nselect count(*) from source_table where full_address like '%Liep%' \n```\n\nResults:\n\t\n| Query\t| csv\t| json\t| avro\t| parquet |\n|-\t| -| -| -| -|    \n| q1\t| 04.711\t| 11.782\t| 00.498\t| 00.103 |\n| q2\t| 12.945\t| 16.360\t| 01.986\t| 03.132 |\n| q3\t| 09.654\t| 14.645\t| 00.795\t| 01.240 |\n| q4\t| 14.365\t| 18.031\t| 02.795\t| 04.463 |\n| q5\t| 08.252\t| 14.008\t| 00.480\t| 00.628 |\n| q6\t| 08.831\t| 14.004\t| 00.544\t| 00.977 |\n\n\n![creat bucket](/assets/basics/file-formats/graph.png)\n\nParquet and Avro are clear winners for running queries\n\n## summary\n\nCSV - can be compressed very well. This format's ancient - so you should not have a problem reading it. \n\n> I would choose this format for moving data via FTP or email.\n\nAvro - I knew nothing about it before starting my tests, it is a row-based format while parquet is columnar. \n\n> I think it would be perfect for storing data consumed by Kafka\n\nJSON - having schema in each row is not efficient... for it, you pay with speed and size.  However at the same time if your schema is evolving you do not need to do anything.\n\n> I still like this format because of how easy it is to deal with it\n\nParquet - I was expecting a little bit more from it (was really surprised by Avro's performance).\n\n> I'm still shocked it was beaten by Avro :D\n\nI think all of these formats will continue to be used.\n\n","top":true,"visible":true,"tags":[{"slug":"storage","title":"Storage","excerpt":"","coverImage":"/assets/logo.png","content":""}]}]},"__N_SSG":true}